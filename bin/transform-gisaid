#!/usr/bin/env python3
"""
Parse the GISAID JSON load into a metadata tsv and a FASTA file.
"""
import regex
import argparse
import sys
from pathlib import Path
import pandas as pd
from typing import List, Optional, Set, Union

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from utils.transform import (
    METADATA_COLUMNS,
    standardize_dataframe,
    fill_default_geo_metadata,
    write_fasta_file,
)

# Preserve the ordering of these columns for ease when generating Slack
# notifications on change
ADDITIONAL_INFO_COLUMNS = [
    'gisaid_epi_isl', 'strain', 'additional_host_info',
    'additional_location_info'
]

assert 'sequence' not in METADATA_COLUMNS, "Sequences should not appear in metadata!"
assert 'sequence' not in ADDITIONAL_INFO_COLUMNS, "Sequences should not appear in additional info!"

def preprocess(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Renames columns and abbreviate strain name in a given *gisaid_data*
    DataFrame, returning the modified DataFrame.
    """
    mapper = {
        'covv_virus_name'       : 'strain',
        'covv_accession_id'     : 'gisaid_epi_isl',
        'covv_collection_date'  : 'date',
        'covv_host'             : 'host',
        'covv_orig_lab'         : 'originating_lab',
        'covv_subm_lab'         : 'submitting_lab',
        'covv_authors'          : 'authors',
        'covv_patient_age'      : 'age',
        'covv_gender'           : 'sex',
        'covv_lineage'          : 'pangolin_lineage',
        'covv_clade'            : 'GISAID_clade',
        'covv_add_host_info'    : 'additional_host_info',
        'covv_add_location'     : 'additional_location_info',
        'covv_subm_date'        : 'date_submitted',
        'covv_location'         : 'location',
    }

    # Calculate sequence length.  GISAID provides a "sequence_length" column,
    # but it sometimes inexplicably goes missing.
    gisaid_data['sequence'] = gisaid_data['sequence'].str.replace('\n', '')
    gisaid_data['length'] = gisaid_data['sequence'].str.len().astype("Int64")

    # Standardize column names, dtypes and drop entries with length less than 15kb
    gisaid_data = standardize_dataframe(gisaid_data, mapper)

    # Abbreviate strain names by removing the prefix. Strip spaces, too.
    gisaid_data['strain'] = gisaid_data['strain'] \
        .str.replace(r'^[hn]CoV-19/', '', n=1, case=False) \
        .str.replace(r'\s', '')

    # Drop duplicates, prefer longest and earliest sequence (assuming accessions
    # are chronological)
    gisaid_data.sort_values(['strain', 'length', 'gisaid_epi_isl'],
        ascending=[True, False, True], inplace=True)

    gisaid_data.drop_duplicates('strain', inplace=True)

    # Sort by strain name
    gisaid_data.sort_values(by=['strain'], inplace=True)

    return gisaid_data

def parse_geographic_columns(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Expands the string found in the column named `location` in the given
    *df*, creating four new columns. Returns the modified ``pd.DataFrame``.
    """
    def titlecase(text: Union[str, pd._libs.missing.NAType],
        articles: Set[str] = {}, abbrev: Set[str] = {}) -> Optional[str]:
        """
        Returns a title cased location name from the given location name
        *tokens*. Ensures that no tokens contained in the *whitelist_tokens* are
        converted to title case.

        >>> articles = {'a', 'and', 'of', 'the', 'le'}
        >>> abbrev = {'USA', 'DC'}

        >>> titlecase("the night OF THE LIVING DEAD", articles)
        'The Night of the Living Dead'

        >>> titlecase("BRAINE-LE-COMTE, FRANCE", articles)
        'Braine-le-Comte, France'

        >>> titlecase("auvergne-RHÔNE-alpes", articles)
        'Auvergne-Rhône-Alpes'

        >>> titlecase("washington DC, usa", articles, abbrev)
        'Washington DC, USA'
        """
        if not isinstance(text, str):
            return

        words = enumerate(regex.split(r'\b', text, flags=regex.V1))

        def changecase(index, word):
            casefold = word.casefold()
            upper = word.upper()

            if upper in abbrev:
                return upper
            elif casefold in articles and index != 1:
                return word.lower()
            else:
                return word.title()

        return ''.join(changecase(i, w) for i, w in words)

    geographic_data = gisaid_data['location'].str.split(r'\s*/\s*', expand=True)
    location_columns = ['region', 'country', 'division', 'location']

    # Manually curate set of tokens that should not be cast to title case
    articles = {
        'de', 'del', 'di', 'do', 'la', 'le', 'las', 'los', 'nad', 'of', 'op',
        'the', 'y'
    }
    abbrev = {'USA', 'DC'}

    for index, column in enumerate(location_columns):
        gisaid_data[column] = geographic_data[index] \
            .str.replace('_', ' ') \
            .str.strip() \
            .apply(lambda text: titlecase(text, articles, abbrev))

    return gisaid_data

def parse_originating_lab(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parses originating lab
    """
    # Strip and normalize whitespace
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace(r'\s+', ' ')
    # Fix common spelling mistakes
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace('Contorl', 'Control')
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace('Dieases', 'Disease')
    return gisaid_data

def parse_submitting_lab(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parses submitting lab
    """
    # Strip and normalize whitespace
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace(r'\s+', ' ')
    # Fix common spelling mistakes
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace('Contorl', 'Control')
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace('Dieases', 'Disease')
    return gisaid_data

def parse_authors(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Abbreviates the column named `authors` to be "<first author> et al" rather
    than a full list.

    This is a "best effort" approach and still mangles a bunch of things.
    Without structured author list data, improvements to the automatic parsing
    meet diminishing returns quickly.  Further improvements should be
    considered using manual annotations of an author map (separate from our
    existing corrections/annotations).
    """
    # Strip and normalize whitespace
    gisaid_data['authors'] = gisaid_data['authors'].str.replace(r'\s+', ' ')
    # Split to first author as best we can for now.  Both commas and semicolons
    # (and their full-width forms) appear to be used interchangeably without a
    # difference of meaning.
    gisaid_data['authors'] = gisaid_data['authors'].str.split(r'(?:\s*[,，;；]\s*|\s+(?:and|&)\s+)').str[0]
    # Add et al
    gisaid_data['authors'] = gisaid_data['authors'].astype(str) + ' et al'
    return gisaid_data

def parse_age(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parse patient age.
    """
    # Convert "60s" or "50's" to "?"
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^\d+\'?[A-Za-z]", '?')
    # Convert to just number
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^(\d+) years$", lambda m: m.group(1))
    # Convert months to years
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^(\d+) months", lambda m: str(int(m.group(1))/12.0))
    # Cleanup unknowns
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^0$", '?')
    # Catch all non-numeric values
    gisaid_data['age'] = pd.to_numeric(gisaid_data['age'], errors='coerce').fillna('?')
    # Convert numeric values to int
    gisaid_data['age'] = gisaid_data['age'].apply(lambda x: x if x=='?' else int(x))
    return gisaid_data

def parse_sex(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parse patient sex.
    """
    # Casing and abbreviations
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^male$", 'Male')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^M$", 'Male')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^female$", 'Female')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^F$", 'Female')
    # Fix spelling
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^Femal$", 'Female')
    # Cleanup unknowns
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^unknown$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^N/A$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^NA$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^not applicable$", '?')
    return gisaid_data

def generate_hardcoded_metadata(hardcoded_metadata: pd.DataFrame) -> pd.DataFrame:
    """
    Returns a ``pd.DataFrame`` with a column for strain ID plus additional
    columns containing harcoded metadata.
    """
    hardcoded_metadata = pd.DataFrame(gisaid_data['strain'])
    hardcoded_metadata['virus']             = 'ncov'
    hardcoded_metadata['genbank_accession'] = '?'
    hardcoded_metadata['url']               = 'https://www.gisaid.org'
    # TODO verify these are all actually true
    hardcoded_metadata['segment']           = 'genome'
    hardcoded_metadata['title']             = '?'
    hardcoded_metadata['paper_url']         = '?'

    return hardcoded_metadata


def update_metadata(curated_gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """ """
    # Add hardcoded metadata which is *almost* always true
    hardcoded_metadata = generate_hardcoded_metadata(curated_gisaid_data)
    curated_gisaid_data.update(hardcoded_metadata)
    curated_gisaid_data = curated_gisaid_data.merge(hardcoded_metadata)

    if args.annotations:
        # Use the curated annotations tsv to update any column values
        user_provided_annotations = pd.read_csv(args.annotations, header=None, sep='\t', comment="#")
        for index, (strain, epi_isl, key, value) in user_provided_annotations.iterrows():
            # Strip any whitespace remaining from inline comments after the final column.
            if isinstance(value, str):
                value = value.rstrip()
            curated_gisaid_data.loc[curated_gisaid_data['gisaid_epi_isl'] == epi_isl, key] = value

    # Fill in blank geographic columns with exisiting geographic data
    curated_gisaid_data = fill_default_geo_metadata(curated_gisaid_data)

    return curated_gisaid_data


if __name__ == '__main__':
    base = Path(__file__).resolve().parent.parent

    parser = argparse.ArgumentParser(
        description="Parse a GISAID JSON load into a metadata tsv and FASTA file.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("gisaid_data",
        default="s3://nextstrain-ncov-private/gisaid.ndjson.gz",
        nargs="?",
        help="Newline-delimited GISAID JSON data")
    parser.add_argument("--annotations",
        default=base / "source-data/gisaid_annotations.tsv",
        help="Optional manually curated annotations TSV.\n"
            "The TSV file should have no header and exactly four columns which contain:\n\t"
            "1. the strain ID (not used for matching; for readability)\n\t"
            "2. the GISAID EPI_ISL accession number (used for matching)\n\t"
            "3. the column name to replace from the generated `metadata.tsv` file\n\t"
            "4. the replacement data\n"
        "Lines or parts of lines starting with '#' are treated as comments.\n"
        "e.g.\n\t"
        "USA/MA1/2020    EPI_ISL_409067    location    Boston\n\t"
        "# First Californian sample\n\t"
        "USA/CA1/2020    EPI_ISL_406034    genbank_accession   MN994467\n\t"
        "Wuhan-Hu-1/2019 EPI_ISL_402125    collection_date 2019-12-26 # Manually corrected date")
    parser.add_argument("--output-metadata",
        default=base / "data/gisaid/metadata.tsv",
        help="Output location of generated metadata tsv. Defaults to `data/metadata.tsv`")
    parser.add_argument("--output-fasta",
        default=base / "data/gisaid/sequences.fasta",
        help="Output location of generated FASTA file. Defaults to `data/sequences.fasta`")
    parser.add_argument("--output-additional-info",
        default=base / "data/gisaid/additional_info.tsv",
        help="Output location of additional info tsv. Defaults to `data/additional_info.tsv`")
    args = parser.parse_args()


    gisaid_data = pd.read_json(args.gisaid_data, lines=True, compression='infer')
    gisaid_data = preprocess(gisaid_data)
    write_fasta_file(gisaid_data, args.output_fasta)

    gisaid_data = parse_geographic_columns(gisaid_data)
    gisaid_data = parse_originating_lab(gisaid_data)
    gisaid_data = parse_submitting_lab(gisaid_data)
    gisaid_data = parse_authors(gisaid_data)
    gisaid_data = parse_age(gisaid_data)
    gisaid_data = parse_sex(gisaid_data)
    gisaid_data = update_metadata(gisaid_data)

    # Export additional info
    gisaid_data[ADDITIONAL_INFO_COLUMNS] \
        .to_csv(args.output_additional_info, sep='\t', na_rep='?', index=False)

    # Reorder columns consistent with the existing metadata on GitHub
    gisaid_data = gisaid_data[METADATA_COLUMNS]
    gisaid_data.to_csv(args.output_metadata, sep='\t', na_rep='', index=False)
