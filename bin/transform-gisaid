#!/usr/bin/env python3
"""
Parse the GISAID NDJSON load into a metadata tsv and a FASTA file.
"""
import argparse
import csv
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Tuple

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from utils.transform import (
    METADATA_COLUMNS,
)
from utils.transformpipeline import LINE_NUMBER_KEY
from utils.transformpipeline.datasource import LineToJsonDataSource
from utils.transformpipeline.filters import LineNumberFilter, SequenceLengthFilter
from utils.transformpipeline.transforms import (
    AbbreviateAuthors,
    AddHardcodedMetadata,
    DropSequenceData,
    ExpandLocation,
    FillDefaultLocationData,
    FixLabs,
    MergeUserAnnotatedMetadata,
    ParsePatientAge,
    ParseSex,
    RenameAndAddColumns,
    StandardizeData,
)

# Preserve the ordering of these columns for ease when generating Slack
# notifications on change
ADDITIONAL_INFO_COLUMNS = [
    'gisaid_epi_isl', 'strain', 'additional_host_info',
    'additional_location_info'
]


assert 'sequence' not in METADATA_COLUMNS, "Sequences should not appear in metadata!"
assert 'sequence' not in ADDITIONAL_INFO_COLUMNS, "Sequences should not appear in additional info!"


if __name__ == '__main__':
    base = Path(__file__).resolve().parent.parent

    parser = argparse.ArgumentParser(
        description="Parse a GISAID JSON load into a metadata tsv and FASTA file.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("gisaid_data",
        default="s3://nextstrain-ncov-private/gisaid.ndjson.gz",
        help="Newline-delimited GISAID JSON data")
    parser.add_argument("--annotations",
        default=base / "source-data/gisaid_annotations.tsv",
        help="Optional manually curated annotations TSV.\n"
            "The TSV file should have no header and exactly four columns which contain:\n\t"
            "1. the strain ID (not used for matching; for readability)\n\t"
            "2. the GISAID EPI_ISL accession number (used for matching)\n\t"
            "3. the column name to replace from the generated `metadata.tsv` file\n\t"
            "4. the replacement data\n"
        "Lines or parts of lines starting with '#' are treated as comments.\n"
        "e.g.\n\t"
        "USA/MA1/2020    EPI_ISL_409067    location    Boston\n\t"
        "# First Californian sample\n\t"
        "USA/CA1/2020    EPI_ISL_406034    genbank_accession   MN994467\n\t"
        "Wuhan-Hu-1/2019 EPI_ISL_402125    collection_date 2019-12-26 # Manually corrected date")
    parser.add_argument("--output-metadata",
        default=base / "data/gisaid/metadata.tsv",
        help="Output location of generated metadata tsv. Defaults to `data/gisaid/metadata.tsv`")
    parser.add_argument("--output-fasta",
        default=base / "data/gisaid/sequences.fasta",
        help="Output location of generated FASTA file. Defaults to `data/gisaid/sequences.fasta`")
    parser.add_argument("--output-additional-info",
        default=base / "data/gisaid/additional_info.tsv",
        help="Output location of additional info tsv. Defaults to `data/gisaid/additional_info.tsv`")
    args = parser.parse_args()

    annotations: Dict[str, List[Tuple[str, Any]]] = defaultdict(list)
    if args.annotations:
        # Use the curated annotations tsv to update any column values
        with open(args.annotations, "r") as gisaid_fh:
            csvreader = csv.reader(gisaid_fh, delimiter='\t')
            for row in csvreader:
                if len(row) != 4:
                    # ensure that it's a comment
                    if row[0].lstrip()[0] != '#':
                        print("WARNING: couldn't decode annotation line " + "\t".join(row))
                    continue
                strain, epi_isl, key, value = row
                annotations[epi_isl].append((
                    key,
                    # remove the comment and the extra ws from the value
                    value.split('#')[0].rstrip(),
                ))

    with open(args.gisaid_data, "r") as gisaid_fh:
        sortable = list(
            LineToJsonDataSource(gisaid_fh)
            | RenameAndAddColumns()
            | StandardizeData()
            | SequenceLengthFilter(15000)
            | DropSequenceData()
        )

    sorted_metadata = sorted(sortable, key=lambda obj: (obj['strain'], -obj['length'], obj['gisaid_epi_isl'], obj[LINE_NUMBER_KEY]))

    # dedup by strain and compile a list of relevant line numbers.
    seen_strains = set()
    line_numbers = set()
    for entry in sorted_metadata:
        if entry['strain'] in seen_strains:
            continue

        seen_strains.add(entry['strain'])
        line_numbers.add(entry[LINE_NUMBER_KEY])

    with open(args.gisaid_data, "r") as gisaid_fh, \
         open(args.output_fasta, "wt") as fasta_fh, \
         open(args.output_additional_info, "wt") as additional_info_fh, \
         open(args.output_metadata, "wt") as metadata_fh:

        # set up the CSV output files
        additional_info_csv = csv.DictWriter(
            additional_info_fh,
            ADDITIONAL_INFO_COLUMNS,
            restval="?",
            extrasaction='ignore',
            delimiter='\t',
        )
        additional_info_csv.writeheader()
        metadata_csv = csv.DictWriter(
            metadata_fh,
            METADATA_COLUMNS,
            restval="?",
            extrasaction='ignore',
            delimiter='\t',
        )
        metadata_csv.writeheader()

        for entry in (
                LineToJsonDataSource(gisaid_fh)
                | RenameAndAddColumns()
                | StandardizeData()
                | SequenceLengthFilter(15000)
                | LineNumberFilter(line_numbers)
                | ExpandLocation()
                | FixLabs()
                | AbbreviateAuthors()
                | ParsePatientAge()
                | ParseSex()
                | AddHardcodedMetadata()
                | MergeUserAnnotatedMetadata(annotations)
                | FillDefaultLocationData()
        ):
            fasta_fh.write(f">{entry['strain']}\n")
            fasta_fh.write(f"{entry['sequence']}\n")
            additional_info_csv.writerow(entry)
            metadata_csv.writerow(entry)
