#!/usr/bin/env python3
"""
Parse the GISAID JSON load into a metadata tsv and a FASTA file.
"""
import argparse
from pathlib import Path
import fsspec
import pandas as pd


# Note: 'sequence' should NEVER appear in these lists!
METADATA_COLUMNS = [  # Ordering of columns in the existing metadata.tsv in the ncov repo
    'strain', 'virus', 'gisaid_epi_isl', 'genbank_accession', 'date', 'region',
    'country', 'division', 'location', 'region_exposure', 'country_exposure',
    'division_exposure', 'segment', 'length', 'host', 'age', 'sex', 'pangolin_lineage', 'GISAID_clade',
    'originating_lab', 'submitting_lab', 'authors', 'url', 'title', 'paper_url',
    'date_submitted'
]

# Preserve the ordering of these columns for ease when generating Slack
# notifications on change
ADDITIONAL_INFO_COLUMNS = [
    'gisaid_epi_isl', 'strain', 'additional_host_info',
    'additional_location_info'
]

assert 'sequence' not in METADATA_COLUMNS, "Sequences should not appear in metadata!"
assert 'sequence' not in ADDITIONAL_INFO_COLUMNS, "Sequences should not appear in additional info!"

def preprocess(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Renames columns and abbreviate strain name in a given *gisaid_data*
    DataFrame, returning the modified DataFrame.
    """
    mapper = {
        'covv_virus_name'       : 'strain',
        'covv_accession_id'     : 'gisaid_epi_isl',
        'covv_collection_date'  : 'date',
        'covv_host'             : 'host',
        'covv_orig_lab'         : 'originating_lab',
        'covv_subm_lab'         : 'submitting_lab',
        'covv_authors'          : 'authors',
        'covv_patient_age'      : 'age',
        'covv_gender'           : 'sex',
        'covv_lineage'          : 'pangolin_lineage',
        'covv_clade'            : 'GISAID_clade',
        'covv_add_host_info'    : 'additional_host_info',
        'covv_add_location'     : 'additional_location_info',
        'covv_subm_date'        : 'date_submitted',
        'covv_location'         : 'location',
    }

    # Standardize to nullable dtypes
    gisaid_data = gisaid_data.convert_dtypes()

    # If an expected field is missing, fill it with NAs so the rest of the
    # script can assume it exists.
    for field in mapper:
        if field not in gisaid_data:
            gisaid_data[field] = pd.NA

    gisaid_data.rename(mapper, axis="columns", inplace=True)

    # Normalize all string columns to Unicode Normalization Form C, for
    # consistent, predictable string comparisons.
    for column in gisaid_data:
        if gisaid_data[column].dtype == "string":
            gisaid_data[column] = gisaid_data[column].str.normalize("NFC").str.strip()

    # Calculate sequence length.  GISAID provides a "sequence_length" column,
    # but it sometimes inexplicably goes missing.
    gisaid_data['sequence'] = gisaid_data['sequence'].str.replace('\n', '')
    gisaid_data['length'] = gisaid_data['sequence'].str.len().astype("Int64")

    # Abbreviate strain names by removing the prefix. Strip spaces, too.
    gisaid_data['strain'] = gisaid_data['strain'] \
        .str.replace(r'^[hn]CoV-19/', '', n=1, case=False) \
        .str.replace(r'\s', '')

    # Drop entries with length less than 15kb
    gisaid_data.drop(gisaid_data[gisaid_data.length < 15000].index, inplace=True)

    # Drop duplicates, prefer longest and earliest sequence (assuming accessions
    # are chronological)
    gisaid_data.sort_values(['strain', 'length', 'gisaid_epi_isl'],
        ascending=[True, False, True], inplace=True)

    gisaid_data.drop_duplicates('strain', inplace=True)

    # Sort by strain name
    gisaid_data.sort_values(by=['strain'], inplace=True)

    return gisaid_data

def parse_geographic_columns(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Expands the string found in the column named `location` in the given
    *df*, creating four new columns. Returns the modified ``pd.DataFrame``.
    """
    geographic_data = gisaid_data['location'].str.split(r'\s*/\s*', expand=True)
    location_columns = ['region', 'country', 'division', 'location']

    for index, column in enumerate(location_columns):
        gisaid_data[column] = geographic_data[index].str.replace('_', ' ').str.strip()

    return gisaid_data

def parse_originating_lab(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parses originating lab
    """
    # Strip and normalize whitespace
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace(r'\s+', ' ')
    # Fix common spelling mistakes
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace('Contorl', 'Control')
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace('Dieases', 'Disease')
    return gisaid_data

def parse_submitting_lab(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parses submitting lab
    """
    # Strip and normalize whitespace
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace(r'\s+', ' ')
    # Fix common spelling mistakes
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace('Contorl', 'Control')
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace('Dieases', 'Disease')
    return gisaid_data

def parse_authors(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Abbreviates the column named `authors` to be "<first author> et al" rather
    than a full list.

    This is a "best effort" approach and still mangles a bunch of things.
    Without structured author list data, improvements to the automatic parsing
    meet diminishing returns quickly.  Further improvements should be
    considered using manual annotations of an author map (separate from our
    existing corrections/annotations).
    """
    # Strip and normalize whitespace
    gisaid_data['authors'] = gisaid_data['authors'].str.replace(r'\s+', ' ')
    # Split to first author as best we can for now.  Both commas and semicolons
    # (and their full-width forms) appear to be used interchangeably without a
    # difference of meaning.
    gisaid_data['authors'] = gisaid_data['authors'].str.split(r'(?:\s*[,，;；]\s*|\s+(?:and|&)\s+)').str[0]
    # Add et al
    gisaid_data['authors'] = gisaid_data['authors'].astype(str) + ' et al'
    return gisaid_data

def parse_age(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parse patient age.
    """
    # Convert "60s" or "50's" to "?"
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^\d+\'?[A-Za-z]", '?')
    # Convert to just number
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^(\d+) years$", lambda m: m.group(1))
    # Convert months to years
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^(\d+) months", lambda m: str(int(m.group(1))/12.0))
    # Cleanup unknowns
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^0$", '?')
    # Catch all non-numeric values
    gisaid_data['age'] = pd.to_numeric(gisaid_data['age'], errors='coerce').fillna('?')
    # Convert numeric values to int
    gisaid_data['age'] = gisaid_data['age'].apply(lambda x: x if x=='?' else int(x))
    return gisaid_data

def parse_sex(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parse patient sex.
    """
    # Casing and abbreviations
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^male$", 'Male')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^M$", 'Male')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^female$", 'Female')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^F$", 'Female')
    # Fix spelling
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^Femal$", 'Female')
    # Cleanup unknowns
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^unknown$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^N/A$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^NA$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^not applicable$", '?')
    return gisaid_data

def generate_hardcoded_metadata(hardcoded_metadata: pd.DataFrame) -> pd.DataFrame:
    """
    Returns a ``pd.DataFrame`` with a column for strain ID plus additional
    columns containing harcoded metadata.
    """
    hardcoded_metadata = pd.DataFrame(gisaid_data['strain'])
    hardcoded_metadata['virus']             = 'ncov'
    hardcoded_metadata['genbank_accession'] = '?'
    hardcoded_metadata['url']               = 'https://www.gisaid.org'
    # TODO verify these are all actually true
    hardcoded_metadata['segment']           = 'genome'
    hardcoded_metadata['title']             = '?'
    hardcoded_metadata['paper_url']         = '?'

    return hardcoded_metadata

def write_fasta_file(sequence_data: pd.DataFrame):
    """ """
    with fsspec.open(str(args.output_fasta), 'wt') as fastafile:
        for index, row in sequence_data.iterrows():
            fastafile.write(f">{row['strain']}\n")
            fastafile.write(f"{row['sequence']}\n")

def update_metadata(curated_gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """ """
    # Add hardcoded metadata which is *almost* always true
    hardcoded_metadata = generate_hardcoded_metadata(curated_gisaid_data)
    curated_gisaid_data.update(hardcoded_metadata)
    curated_gisaid_data = curated_gisaid_data.merge(hardcoded_metadata)

    if args.annotations:
        # Use the curated annotations tsv to update any column values
        user_provided_annotations = pd.read_csv(args.annotations, header=None, sep='\t', comment="#")
        for index, (strain, epi_isl, key, value) in user_provided_annotations.iterrows():
            # Strip any whitespace remaining from inline comments after the final column.
            if isinstance(value, str):
                value = value.rstrip()
            curated_gisaid_data.loc[curated_gisaid_data['gisaid_epi_isl'] == epi_isl, key] = value

    # if division is blank, replace with country data, to avoid unexpected effects when subsampling by division
    # (where an empty division is counted as a 'division' group)
    curated_gisaid_data.loc[pd.isnull(curated_gisaid_data['division']), 'division'] = curated_gisaid_data['country']

    # Set `region_exposure` equal to `region` if it wasn't added by annotations
    if 'region_exposure' in curated_gisaid_data:
        curated_gisaid_data['region_exposure'].fillna(curated_gisaid_data['region'], inplace=True)
    else:
        curated_gisaid_data['region_exposure'] = curated_gisaid_data['region']

    # Set `country_exposure` equal to `country` if it wasn't added by annotations
    if 'country_exposure' in curated_gisaid_data:
        curated_gisaid_data['country_exposure'].fillna(curated_gisaid_data['country'], inplace=True)
    else:
        curated_gisaid_data['country_exposure'] = curated_gisaid_data['country']

    # Set `division_exposure` equal to `division` if it wasn't added by annotations
    if 'division_exposure' in curated_gisaid_data:
        curated_gisaid_data['division_exposure'].fillna(curated_gisaid_data['division'], inplace=True)
    else:
        curated_gisaid_data['division_exposure'] = curated_gisaid_data['division']

    return curated_gisaid_data


if __name__ == '__main__':
    base = Path(__file__).resolve().parent.parent

    parser = argparse.ArgumentParser(
        description="Parse a GISAID JSON load into a metadata tsv and FASTA file.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("gisaid_data",
        default="s3://nextstrain-ncov-private/gisaid.ndjson.gz",
        nargs="?",
        help="Newline-delimited GISAID JSON data")
    parser.add_argument("--annotations",
        default=base / "source-data/annotations_gisaid.tsv",
        help="Optional manually curated annotations TSV.\n"
            "The TSV file should have no header and exactly four columns which contain:\n\t"
            "1. the strain ID (not used for matching; for readability)\n\t"
            "2. the GISAID EPI_ISL accession number (used for matching)\n\t"
            "3. the column name to replace from the generated `metadata.tsv` file\n\t"
            "4. the replacement data\n"
        "Lines or parts of lines starting with '#' are treated as comments.\n"
        "e.g.\n\t"
        "USA/MA1/2020    EPI_ISL_409067    location    Boston\n\t"
        "# First Californian sample\n\t"
        "USA/CA1/2020    EPI_ISL_406034    genbank_accession   MN994467\n\t"
        "Wuhan-Hu-1/2019 EPI_ISL_402125    collection_date 2019-12-26 # Manually corrected date")
    parser.add_argument("--output-metadata",
        default=base / "data/metadata_gisaid.tsv",
        help="Output location of generated metadata tsv. Defaults to `data/metadata.tsv`")
    parser.add_argument("--output-fasta",
        default=base / "data/sequences_gisaid.fasta",
        help="Output location of generated FASTA file. Defaults to `data/sequences.fasta`")
    parser.add_argument("--output-additional-info",
        default=base / "data/additional_info_gisaid.tsv",
        help="Output location of additional info tsv. Defaults to `data/additional_info.tsv`")
    args = parser.parse_args()


    gisaid_data = pd.read_json(args.gisaid_data, lines=True, compression='infer')
    gisaid_data = preprocess(gisaid_data)
    write_fasta_file(gisaid_data)

    gisaid_data = parse_geographic_columns(gisaid_data)
    gisaid_data = parse_originating_lab(gisaid_data)
    gisaid_data = parse_submitting_lab(gisaid_data)
    gisaid_data = parse_authors(gisaid_data)
    gisaid_data = parse_age(gisaid_data)
    gisaid_data = parse_sex(gisaid_data)
    gisaid_data = update_metadata(gisaid_data)

    # Export additional info
    gisaid_data[ADDITIONAL_INFO_COLUMNS] \
        .to_csv(args.output_additional_info, sep='\t', na_rep='?', index=False)

    # Reorder columns consistent with the existing metadata on GitHub
    gisaid_data = gisaid_data[METADATA_COLUMNS]
    gisaid_data.to_csv(args.output_metadata, sep='\t', na_rep='', index=False)
